{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: change Jupyter Notebook theme to GDD theme\n",
    "from IPython.core.display import HTML\n",
    "HTML(url='https://gdd.li/jupyter-theme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![footer_logo](images/logo.png)\n",
    "# Seasonality Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "During this session we shall focus on modelling Time Series data. \n",
    "\n",
    "We shall learn about the different components we can identify in Time Series data and how they can be modelled.\n",
    "\n",
    "We shall also discuss how a Time Series model can be used for outlier detection and how to evaluate our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program\n",
    "\n",
    "1. [Time Series Decomposition](#ets)\n",
    "2. [Linear Modeling Approach](#lma)\n",
    "3. [Dealing with Seasonality](#dws)\n",
    "4. [Gradual Seasonal Filtering](#gsf)\n",
    "5. [Outlier Detection](#od)\n",
    "6. [Forecast Evaluation for Seasonality Models](#eval)\n",
    "7. [Summary](#sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![footer_logo](images/air-quality.jpeg) \n",
    "\n",
    "\n",
    "## The Data\n",
    "\n",
    "We will again use a dataset containing daily air quality index in Californian counties between 2007 and 2017 (based on a larger dataset from [Kaggle](https://www.kaggle.com/epa/carbon-monoxide)). \n",
    "\n",
    "Each datapoint indicates the average air quality index on a certain day: the higher the value - the more polluted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "air_df = pd.read_csv('data/air_quality.csv', index_col='date_local', parse_dates=True)\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualising the data we can we can identify some patterns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These regularities in the data can be categorised as follows:\n",
    "- **Trends** (upward / horizontal / downward)\n",
    "- **Seasonality** (predictably repeating cycles - weekly/monthly/yearly etc)\n",
    "- **Cyclical components** (patterns with no period - for example trend breaks) \n",
    "- **Residuals** (the remaining part of the series that cannot be further explicitly modeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ets'></a>\n",
    "## 1. Time Series decomposition\n",
    "\n",
    "Seasonality is very common in business data. However, it can obscure the actual signal of the data, which complicates both understanding of the underlying processes and further forecasting. \n",
    "\n",
    "Accordingly, we may wish to separate Time Series data into its trend and seasonal components. This process is known as **Time Series decomposition**.\n",
    "\n",
    "One of the simplest ways to identify the general trend is to substantially **smooth** the Time Series data. For example, the smoothed data clearly suggests a downward trend in AQI (or that air quality is improving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    air_df\n",
    "    .assign(rolling = lambda df: df['aqi'].rolling(365, center=True).mean(),\n",
    "            #exponential=lambda df: df['aqi'].ewm(alpha=0.001).mean()\n",
    "           )\n",
    "    .plot()\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we naively assume that the remaining variation is due to seasonality and noise, we can isolate this from the Time Series data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (\n",
    "    air_df\n",
    "    .assign(rolling = lambda df: df['aqi'].rolling(365, center=True).mean(),\n",
    "            seasonality_noisey = lambda df: df['aqi'] - df['rolling'],\n",
    "            #seasonality_smoothed = lambda df: df['seasonality_noisey'].rolling(90, center=True).mean(),\n",
    "            #error = lambda df: df['aqi'] - df['rolling'] - df['seasonality_smoothed']\n",
    "           )\n",
    "    [['seasonality_noisey']]\n",
    "    #[['seasonality_noisey', 'seasonality_smoothed']]\n",
    "    .plot()\n",
    ")\n",
    "ax.axhline(0, color='r', linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we still have the seasonal waves but without the downwards trend.\n",
    "\n",
    "However, whilst smoothing can average out the effects of seasonality and noise, it does not provide us with a mathematical model that can describe the data and be used for forecasting. \n",
    "\n",
    "In addition to this the \"centering\" it involves makes use of information from the future, which would not be possible in the context of forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lma'></a>\n",
    "## 2. Linear Modeling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to identify the main pattern(s) taking place in the data is to fit a linear regression. \n",
    "\n",
    "Even the most basic linear model with a single time component can inform us about the general trend and allow us to (mostly) separate it from the seasonality.\n",
    "\n",
    "To demonstrate this we shall learn a linear regression model that is able to predict the AQI of a particular date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first create a variable to indicate which number time point the dates represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df['time_point'] = np.arange(len(air_df))\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then separate the data into a feature matrix **X** and target vetor **y** so we can fit a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = air_df[['time_point']]\n",
    "y = air_df['aqi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions from this model represents the linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df['linear_trend'] = lm.predict(X)\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the outcome for the model on the data it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "air_df[['aqi','linear_trend']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As simple as this is, it gives us an idea of what happens over time. \n",
    "\n",
    "Of course a single linear trend cannot take dynamical changes in growth/decline rates into account. \n",
    "\n",
    "For example the trend appears to change around summer 2014.\n",
    "\n",
    "<!-- https://en.wikipedia.org/wiki/Hurricane_Marie_(2014) -->\n",
    "\n",
    "But we can take care of this by adding break indicators & interaction terms:\n",
    "- The `after_summer_2014` indicator term will allows us to add a particular quantity depending on whether we are before or after the date 01/08/2014.\n",
    "- The `interaction` term allows to add an additional quantities which are dependent on the time point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_break = (\n",
    "    air_df\n",
    "    .assign(after_summer_2014 = np.where(air_df.index > pd.Timestamp('2014-08'), 1, 0),\n",
    "            interaction = lambda df: df['time_point']*df['after_summer_2014'])\n",
    "    [['time_point', 'after_summer_2014','interaction']]\n",
    ")\n",
    "X_break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can use these features to make more informed predictions about the linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_break = LinearRegression().fit(X_break, y)\n",
    "\n",
    "air_df['linear_trend_break'] = lm_break.predict(X_break)\n",
    "\n",
    "air_df[['aqi','linear_trend_break']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dws'></a>\n",
    "## 3. Dealing with Seasonality\n",
    "\n",
    "We may want to do more than just identifying the trend though. Modeling the seasonality would allow us to model not just the average behavior, but exact values during each season. It would also allow us to quantify the seasonal effects too.\n",
    "\n",
    "A simple way to achieve this would be to add seasonal <font color='red'>dummy terms</font> to the baseline linear regression. This is known as <font color='red'>feature engineering</font> and can be a very powerful tool in Time Series analysis, allowing us to capture rather complex patterns with a few simple engineered variables added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall add a feature indicating what month the dates are in and use that to help with our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df['month'] = air_df.index.month\n",
    "X_monthly = air_df[['time_point','month']]\n",
    "X_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall also perform a train-test split so that we can evaluate our forecasting predictions later on. \n",
    "\n",
    "The train-test split is unsuffled to ensure that the test set data comes after the train set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_monthly_train, X_monthly_test, y_train, y_test = train_test_split(X_monthly, y, shuffle=False, train_size=len(X[:'2014']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "y_train.plot(ax=ax)\n",
    "y_test.plot(ax=ax)\n",
    "plt.legend([\"train set\",\"test set\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We one-hot encode the month information as it is a categorical value and then perform linear regression.\n",
    "\n",
    "<!-- We do drop first because otherwise we will have co-linearites: high-correlation between the predictor variables, which can lead to problems.\n",
    "- 1. Redundancy: two predictors might be providing the same information about the response variable.\n",
    "- 2. The estimate of the effect a predictor on the response variable will tend to be less precise and less reliable.\n",
    "- 3. An important predictor can become unimportant as that feature has a collinear relationship with other predictor.\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "encoder = ColumnTransformer(\n",
    "      [('categorical', OneHotEncoder(drop='first'), ['month'])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "lm_monthly = Pipeline([\n",
    "    ('preprocess', encoder),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "lm_monthly.fit(X_monthly_train, y_train)\n",
    "air_df['linear_monthly'] = lm_monthly.predict(X_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "air_df[['aqi','linear_monthly']].loc[:'2014'].plot(color=['#499DE6','red']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple approach it is possible to separate the trend and seasonality components of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t'></a>\n",
    "### Trend\n",
    "\n",
    "The coeffiecents for the monthly features indicate the seasonal effect they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    columns = calendar.month_name[2:],\n",
    "    data = [lm_monthly['model'].coef_[:-1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To isolate the trend we can replace the seasonal coefficients with their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_seasonal_effect = lm_monthly['model'].coef_[:-1].sum()/12\n",
    "average_seasonal_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this average seasonal effect in the regression, inplace of the individual coefficients, allows us to get the trend line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    air_df\n",
    "    .assign(trend = lambda df: lm_monthly['model'].coef_[-1]*df['time_point'] \n",
    "                                + average_seasonal_effect \n",
    "                                + lm_monthly['model'].intercept_,\n",
    "           )\n",
    "    [['aqi','linear_monthly', 'trend']]\n",
    "    .loc[:'2014']\n",
    "    .plot(color=['#499DE6','red', 'green'])\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='s'></a>\n",
    "### Seasonality\n",
    "\n",
    "Subtracting the trend from the predictions allows us to separate the seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (\n",
    "    air_df\n",
    "    .assign(residuals = lambda df: df['aqi'] - df['linear_monthly'],\n",
    "            trend = lambda df: lm_monthly['model'].coef_[-1]*df['time_point'] \n",
    "                                + average_seasonal_effect \n",
    "                                + lm_monthly['model'].intercept_,\n",
    "            seasonality = lambda df: df['linear_monthly']-df['trend']\n",
    "           )\n",
    "    ['seasonality']\n",
    "    .loc[:'2014']\n",
    "    .plot(c='m')\n",
    ")\n",
    "ax.axhline(0, color='r', linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we still have the seasonal waves but without the downwards trend.\n",
    "\n",
    "This simple model with dummy features appears to reasonably capture the observed seasonality, even if has limitations. For one, the model assumes fixed monthly jumps, while the actual seasonality is likely to more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gsf'></a>\n",
    "## 4. Gradual Seasonal Filtering\n",
    "\n",
    "Often seasonal effects do not come just as fixed spikes or drops. They may have gradually increasing and decreasing effects as their peak approaches and moves away in time. In such cases we may want to use a neater alternative to seasonal dummies: <font color='red'>gradual seasonal filters</font>.\n",
    "\n",
    "There are a variety of such features that we could create. \n",
    "\n",
    "### Linear\n",
    "\n",
    "A simple yet effective example are linear monthly spikes. They can be computed like so, \n",
    "\n",
    "$$ \\phi(x_i) = \\max( 1 - \\frac{| x - x^*|}{n} , 0)$$\n",
    "\n",
    "where $x$ is a given data point, $x^*$ is the peak of the current filter, and $n$ is the interval of growth/decline of the spike around the peak (e.g. 30 days). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsf_feature_maker(center_day, days=np.arange(365), year_days=365, n=30):\n",
    "    return np.maximum.reduce([\n",
    "        np.fmax(-np.abs(days - center_day)/n + 1, 0),\n",
    "        np.fmax(-np.abs(days + year_days - center_day)/n + 1, 0),  #ensures continuity across December-January\n",
    "        np.fmax(-np.abs(days - year_days - center_day)/n + 1, 0) #ensures continuity across December-January\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a filter like this translates into a variable with values the closer to 1 (or to 0), the closer (or the further) a particular date is from the given peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = gsf_feature_maker(10)\n",
    "plt.plot(base);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such gradual filters could be especially handy when dealing with daily data that typically exhibits more gradual seasonal effects. \n",
    "\n",
    "A separate filter can be assigned to each potential seasonal peak â€” each month in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_2008 = pd.date_range('2008-01-01', periods = 12, freq='MS')\n",
    "\n",
    "month_peaks = pd.Series(\n",
    "    index = months_2008.map(lambda data: data.month_name()),\n",
    "    data = months_2008.map(lambda data: data.replace(day=15).dayofyear)\n",
    ")\n",
    "\n",
    "month_peaks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for month in month_peaks.index:\n",
    "    peak = month_peaks.at[month]\n",
    "    ax.plot(gsf_feature_maker(peak), label=month)\n",
    "ax.legend(loc='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this information as features and use it to model the Time Series data. \n",
    "\n",
    "It means that the effect is not felt uniformly across a month, but increases/decreases depending on the distance from the desginated centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gsf = air_df[['time_point']].copy()\n",
    "\n",
    "days = X_gsf.index.dayofyear\n",
    "\n",
    "for month in month_peaks.index:\n",
    "    peak = month_peaks.at[month]\n",
    "    X_gsf[month] = gsf_feature_maker(peak, days)\n",
    "    \n",
    "X_gsf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian\n",
    "\n",
    "Another commonly used option are filters based on a *Gaussian distribution*:\n",
    "\n",
    "$$ \\phi(x_i) = \\exp [ - \\frac{1}{2\\alpha} (x-m_i)^2]$$\n",
    "\n",
    "where $x$ is a given data point, $m_i$ is the peak of the current filter, and $\\alpha$ is a parameter responsible for the spread of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_feature_maker(center_day, days=np.arange(365), year_days = 365, alpha = 0.005):\n",
    "    return np.maximum.reduce([\n",
    "        np.fmax(np.exp(-((days - center_day)**2) / 2*alpha), 0),\n",
    "        np.fmax(np.exp(-((days + year_days - center_day)**2) / 2*alpha), 0), #ensures continuity for December-January\n",
    "        np.fmax(np.exp(-((days - year_days - center_day)**2) / 2*alpha), 0)  #ensures continuity for December-January\n",
    "        ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a filter like this translates into a variable with values the closer to 1 (or to 0), the closer (or the further) a particular date is from the given peak. However, the effect is much smoother.\n",
    "\n",
    "Depending on $\\alpha$, very steep or very gradual filters can be created.  Such gradual filters could be especially handy when dealing with daily (or even hourly) data that typically exhibits more gradual seasonal effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = rbf_feature_maker(10, alpha = 0.005)\n",
    "plt.plot(base);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exercise: Modelling with Gradual Seasonal Filters</mark>\n",
    "\n",
    "Add Gaussian features that correspond to the different months and use them to help model the data. \n",
    "- How do the preditions compare visually to the model with seasonal dummy features?\n",
    "- How is the performance affected by the $\\alpha$ value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/rbf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradual Seasonal Filtering summary\n",
    "\n",
    "**Pros** \n",
    "\n",
    "- simple feature engineering trick\n",
    "- all variables are interpretable\n",
    "- seasonal effects can be quantified\n",
    "- focus on filtering out the long term season, other fluctuations can be modeled separately\n",
    "\n",
    "**Cons** \n",
    "\n",
    "- the model can get a bit biased \n",
    "- the model may have issues if the seasonality changes over time (fixable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sine'></a>\n",
    "### BONUS: Fitting a Sinusoid Curve?\n",
    "\n",
    "An alternative approach to using seasonal dummies or other similar features could be fitting a predetermined function that naturally mimics the behavior of our Time Series. In fact our data kind of looks like sine fluctuations, doesn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "air_df['sine_example']=2*np.sin(0.017*air_df['time_point'])\n",
    "\n",
    "air_df[['aqi', 'sine_example']].plot(color=['#499DE6','red']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how can we fit a sine curve? Is it still possible to use linear regression?\n",
    "\n",
    "A standard linear model fitting a sine curve over a year would have the following form:\n",
    "\n",
    "$$ y_i = w_0 + w_1 t_i + w_2 \\sin{(\\frac{2 \\pi t_i}{365} + \\Phi)} + \\varepsilon_i$$\n",
    "\n",
    "where $w_2$ determines the amplitude (gap between top and bottom parts) and $\\Phi$ shifts the sine curve left/right along the x-axis. \n",
    "\n",
    "The problem with this though is that there are two parameters to learn ($w_2$ & $\\Phi$) for a single linear component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately we can use a very neat trick from Trigonometry that will allow us to separate the two:\n",
    "\n",
    "$$ \\sin{(a - b)} = \\sin{(a)}\\cos{(b)} - \\cos{(a)}\\sin{(b)}$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$ \\sin{(2 \\pi t_i + \\Phi)} = \\sin{(\\frac{2 \\pi t_i}{365})}\\cos{(\\Phi)} - \\cos{(\\frac{2 \\pi t_i}{365})}\\sin{(\\Phi)}$$\n",
    "\n",
    "Since $\\sin{(\\Phi)}$ and $\\cos{(\\Phi)}$ are just constants, we can incorporate them into the coefficients and simply fit the following linear model:\n",
    "\n",
    "$$ y_i = w_0 + w_1 t_i + w_2 \\sin{(\\frac{2 \\pi t_i}{365})} + w_3 \\cos{(\\frac{2 \\pi t_i}{365})} + \\varepsilon_i$$\n",
    "\n",
    "We simply need to calculate the $\\sin$ and $\\cos$ components as new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trig = (\n",
    "    air_df[['time_point']]\n",
    "    .assign(sin = lambda df: np.sin(2*np.pi*df['time_point']/365),\n",
    "            cos = lambda df: np.cos(2*np.pi*df['time_point']/365),\n",
    "#             sin_time = lambda df: df['sin']*df['time_point'],\n",
    "#             cos_time = lambda df: df['cos']*df['time_point']        \n",
    "    )\n",
    "\n",
    ")\n",
    "X_trig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_trig = LinearRegression()\n",
    "\n",
    "X_trig_train = X_trig.loc[:'2014']\n",
    "X_trig_test = X_trig.loc['2015':]\n",
    "\n",
    "lm_trig.fit(X_trig_train, y_train)\n",
    "\n",
    "air_df['linear_trig'] = lm_trig.predict(X_trig)\n",
    "\n",
    "air_df[['aqi','linear_trig']].loc[:'2014'].plot(color=['#499DE6','red']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks quite naural, however, one remaining problem is that the fluctuations appear to shrink over time. \n",
    "\n",
    "*How could we fix this?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sinusoidal fitting summary\n",
    "\n",
    "Fitting a sine curve can be an example of a high bias & low variance seasonality model. We have less risk of overfitting, but perhaps we oversimplify the dynamics too much. Still, it may be a good choice for models with very regular seasonal cycles or simply when we require quick deseasonalization.\n",
    "\n",
    "This example also serves as an introduction to a more advanced technique - [Fast Fourier Transform algorithm](https://ipython-books.github.io/101-analyzing-the-frequency-components-of-a-signal-with-a-fast-fourier-transform/), that relies on combining multiple sine terms for seasonality modeling. It uses the famous result by French mathematician Joseph Fourier: \n",
    "\n",
    "> \"a reasonably continuous and periodic function can be expressed as the sum of series of sine terms.\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    air_df\n",
    "    .assign(residuals = lambda df: df['aqi'] - df['linear_trig'],\n",
    "            trend = lambda df: lm_trig.coef_[0]*air_df['time_point'] + lm_trig.intercept_,\n",
    "            seasonality = lambda df: df['linear_monthly']-df['trend']\n",
    "           )\n",
    "    [['aqi','linear_trig','trend', 'seasonality', 'residuals']]\n",
    "    .loc[:'2015-4']\n",
    "    .plot(legend=False, color=['#499DE6','red', 'green', 'orangered', 'slategrey'])\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='od'></a>\n",
    "## 5. Outlier detection\n",
    "\n",
    "Having a reasonable model for seasonality, we can now also use it to better understand what was happening in the past. Most notably, we can identify periods that stand out from regular seasonal patterns. This brings us to outlier detection. While there are many techniques for that, a simple natural way would be to look for data points deviating from the model more than a certain threshold percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df['outliers'] = np.where(((air_df['aqi'] - air_df['linear_rbf'])/air_df['linear_rbf']).abs() > 0.15,\n",
    "                               air_df['aqi'],\n",
    "                               np.nan)\n",
    "air_df.loc[air_df['outliers'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "air_df[['aqi','linear_rbf']].loc[:'2014'].plot(color=['#499DE6','green'], ax=ax, alpha=0.7)\n",
    "air_df.loc[:'2014'].reset_index().plot(color='orangered', marker='.', ax=ax, \n",
    "                                       kind='scatter', x='date_local', y='outliers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the outliers year by year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "year_to_examine= '2014'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "air_df[['aqi','linear_rbf']].loc[year_to_examine].plot(color=['#499DE6','green'], ax=ax, alpha=0.7)\n",
    "air_df.loc[year_to_examine].reset_index().plot(color='orangered', ax=ax, kind='scatter', x='date_local', y='outliers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking against a certain percentage threshold may be a bit arbitrary. That's why more advanced approaches to outlier detection use confidence bounds around the model instead. The example above should nonetheless illustrate the idea of how such techniques work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval'></a>\n",
    "## 6. Forecast Evaluation for Seasonality Models\n",
    "\n",
    "So far we have only assessed the performance of our models visually on past data (the data they were trained on). \n",
    "\n",
    "In practice, we want to use these models for forecasting. It would therefore be useful to compare the performance of the models on unseen data from the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    air_df\n",
    "    .loc['2015':]\n",
    "    [['aqi', 'linear_monthly', 'linear_rbf', 'linear_trig']]\n",
    "    .plot(color=['#499DE6','orangered', 'red', 'green'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our models are better at predicting winter values rather than summer values.\n",
    "\n",
    "The predictions are off for Summer 2016 in particular. Why might this be case?\n",
    "\n",
    "Let's evaluate the Mean Absolute Percentage Error (MAPE) of each model and see which one actually did the best.\n",
    "\n",
    "MAPE is a commonly used metric for evaluting time series data as it is scale independent, so it can be used to compare forecast performance between different Time Series (and different sections of Time Series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "print(f\"MAPE for the model with dummy feaures is \\\n",
    "{round(mean_absolute_percentage_error(y_test, air_df['linear_monthly'].loc['2015':])*100,2)}\")\n",
    "\n",
    "print(f\"MAPE for the (Gaussian) gsf model is \\\n",
    "{round(mean_absolute_percentage_error(y_test, air_df['linear_rbf'].loc['2015':])*100,2)}\")\n",
    "\n",
    "# print(f\"MAPE for the sinusoid model is \\\n",
    "# {round(mean_absolute_percentage_error(y_test, air_df['linear_trig'].loc['2015':])*100,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the model with seasonal dummies appears to perform (slightly) better than the GSF model. \n",
    "\n",
    "This should make us thinking and raise a few questions for the future:\n",
    "\n",
    "- Are we using an adequate metric?\n",
    "- Do we want to compare forecasts the the real noisy data or to smoothed series?\n",
    "- How far in the future we want to forecast?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sum'></a>\n",
    "## 7. Summary\n",
    "\n",
    "Whilst modelling and forecasting time series data we have come across a few key points:\n",
    "- Seasonality matters!\n",
    "- Feature engineering is a way to turn your creativity into better models\n",
    "- There are many ways to identify it\n",
    "- Linear models are more powerful than people think\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
