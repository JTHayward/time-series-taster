{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<link rel=\"stylesheet\" href=\"https://marysia.nl/assets/GDD/css/custom.css\">\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: change Jupyter Notebook theme to GDD theme\n",
    "from IPython.core.display import HTML\n",
    "HTML(url='https://gdd.li/jupyter-theme')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![footer_logo](images/logo.png)\n",
    "# Seasonality Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "During this session we shall focus on modelling Time Series data. \n",
    "\n",
    "We shall learn about the different components we can identify in Time Series data and how they can be modelled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program\n",
    "\n",
    "1. [Time Series Decomposition](#ets)\n",
    "2. [Linear Modeling Approach](#lma)\n",
    "3. [Dealing with Seasonality](#dws)\n",
    "4. [Gradual Seasonal Filtering](#gsf)\n",
    "6. [Forecast Evaluation for Seasonality Models](#eval)\n",
    "7. [Summary](#sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![footer_logo](images/air-quality.jpeg) \n",
    "\n",
    "\n",
    "## The Data\n",
    "\n",
    "We will again use a dataset containing daily air quality index in Californian counties between 2007 and 2017 (based on a larger dataset from [Kaggle](https://www.kaggle.com/epa/carbon-monoxide)). \n",
    "\n",
    "Each datapoint indicates the average air quality index on a certain day: the higher the value - the more polluted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "air_df = pd.read_csv('data/air_quality.csv', index_col='date_local', parse_dates=True)\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By visualising the data we can we can identify some patterns,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These regularities in the data can be categorised as follows:\n",
    "- **Trends** (upward / horizontal / downward)\n",
    "- **Seasonality** (predictably repeating cycles - weekly/monthly/yearly etc)\n",
    "- **Cyclical components** (patterns with no period - for example trend breaks) \n",
    "- **Residuals** (the remaining part of the series that cannot be further explicitly modeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ets'></a>\n",
    "## 1. Time Series decomposition\n",
    "\n",
    "Seasonality is very common in business data. However, it can obscure the actual signal of the data, which complicates both understanding of the underlying processes and further forecasting. \n",
    "\n",
    "Accordingly, we may wish to separate Time Series data into its trend and seasonal components. This process is known as **Time Series decomposition**.\n",
    "\n",
    "One of the simplest ways to identify the general trend is to substantially **smooth** the Time Series data. For example, the smoothed data clearly suggests a downward trend in AQI (or that air quality is improving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    air_df\n",
    "    .assign(rolling = lambda df: df['aqi'].rolling(365, center=True).mean(),\n",
    "            #exponential=lambda df: df['aqi'].ewm(alpha=0.001).mean()\n",
    "           )\n",
    "    .plot()\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, whilst smoothing can average out the effects of seasonality and noise, it does not provide us with a mathematical model that can describe the data and be used for forecasting. \n",
    "\n",
    "In addition to this the \"centering\" it involves makes use of information from the future, which would not be possible in the context of forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lma'></a>\n",
    "## 2. Linear Modeling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to identify the main pattern(s) taking place in the data is to fit a linear regression. \n",
    "\n",
    "Even the most basic linear model with a single time component can inform us about the general trend and allow us to (mostly) separate it from the seasonality.\n",
    "\n",
    "To demonstrate this we shall learn a linear regression model that is able to predict the AQI of a particular date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first create a variable to indicate which number time point the dates represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df['time_point'] = np.arange(len(air_df))\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then separate the data into a feature matrix **X** and target vetor **y** so we can fit a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = air_df[['time_point']]\n",
    "y = air_df['aqi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions from this model represents the linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df['linear_trend'] = lm.predict(X)\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the outcome for the model on the data it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "air_df[['aqi','linear_trend']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As simple as this is, it gives us an idea of what happens over time. \n",
    "\n",
    "Of course a single linear trend cannot take dynamical changes in growth/decline rates into account. \n",
    "\n",
    "For example the trend appears to change around summer 2014.\n",
    "\n",
    "<!-- https://en.wikipedia.org/wiki/Hurricane_Marie_(2014) -->\n",
    "\n",
    "But we can take care of this by adding break indicators & interaction terms:\n",
    "- The `after_summer_2014` indicator term will allows us to add a particular quantity depending on whether we are before or after the date 01/08/2014.\n",
    "- The `interaction` term allows to add an additional quantities which are dependent on the time point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_break = (\n",
    "    air_df\n",
    "    .assign(after_summer_2014 = np.where(air_df.index > pd.Timestamp('2014-08'), 1, 0),\n",
    "            interaction = lambda df: df['time_point']*df['after_summer_2014'])\n",
    "    [['time_point', 'after_summer_2014','interaction']]\n",
    ")\n",
    "X_break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can use these features to make more informed predictions about the linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_break = LinearRegression().fit(X_break, y)\n",
    "\n",
    "air_df['linear_trend_break'] = lm_break.predict(X_break)\n",
    "\n",
    "air_df[['aqi','linear_trend_break']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dws'></a>\n",
    "## 3. Dealing with Seasonality\n",
    "\n",
    "We may want to do more than just identifying the trend though. Modeling the seasonality would allow us to model not just the average behavior, but exact values during each season. It would also allow us to quantify the seasonal effects too.\n",
    "\n",
    "A simple way to achieve this would be to add seasonal <font color='red'>dummy terms</font> to the baseline linear regression. This is known as <font color='red'>feature engineering</font> and can be a very powerful tool in Time Series analysis, allowing us to capture rather complex patterns with a few simple engineered variables added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall add a feature indicating what month the dates are in and use that to help with our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df['month'] = air_df.index.month_name()\n",
    "X_monthly = air_df[['time_point','month']]\n",
    "X_monthly.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to numerically encode the month information before we can learn a model.\n",
    "\n",
    "Ordinally encoding the months would enforce incorrect biases. \n",
    "\n",
    "So instead, we will one-hot encode the categories so that each month has it's own coefficient.\n",
    "\n",
    "<!-- We do drop first because otherwise we will have co-linearites: high-correlation between the predictor variables, which can lead to problems.\n",
    "- 1. Redundancy: two predictors might be providing the same information about the response variable.\n",
    "- 2. The estimate of the effect a predictor on the response variable will tend to be less precise and less reliable.\n",
    "- 3. An important predictor can become unimportant as that feature has a collinear relationship with other predictor.\n",
    " -->\n",
    " \n",
    "We can then perform linear regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "encoder = ColumnTransformer(\n",
    "      [('categorical', OneHotEncoder(drop='first'), ['month'])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "lm_monthly = Pipeline([\n",
    "    ('preprocess', encoder),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "lm_monthly.fit(X_monthly, y)\n",
    "air_df['linear_monthly'] = lm_monthly.predict(X_monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "air_df[['aqi','linear_monthly']].loc[:'2014'].plot(color=['#499DE6','red']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple approach it is possible to separate the trend and seasonality components of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='t'></a>\n",
    "### Trend\n",
    "\n",
    "The coeffiecents for the monthly features indicate the seasonal effect they have.\n",
    "\n",
    "<!-- *If it is February we add $-1.1$ to our predictions, if it's March we add $-2.2$, but if it's January we add 0.* -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    columns = calendar.month_name[2:],\n",
    "    data = [lm_monthly['model'].coef_[:-1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To isolate the trend we can replace the seasonal coefficients with their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_seasonal_effect = lm_monthly['model'].coef_[:-1].sum()/12\n",
    "average_seasonal_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this average seasonal effect in the regression, inplace of the individual coefficients, allows us to get the trend line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    air_df\n",
    "    .assign(trend = lambda df: lm_monthly['model'].coef_[-1]*df['time_point'] \n",
    "                                + average_seasonal_effect \n",
    "                                + lm_monthly['model'].intercept_,\n",
    "           )\n",
    "    [['aqi','linear_monthly', 'trend']]\n",
    "    .loc[:'2014']\n",
    "    .plot(color=['#499DE6','red', 'green'])\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='s'></a>\n",
    "### Seasonality\n",
    "\n",
    "Subtracting the trend from the predictions allows us to separate the seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (\n",
    "    air_df\n",
    "    .assign(residuals = lambda df: df['aqi'] - df['linear_monthly'],\n",
    "            trend = lambda df: lm_monthly['model'].coef_[-1]*df['time_point'] \n",
    "                                + average_seasonal_effect \n",
    "                                + lm_monthly['model'].intercept_,\n",
    "            seasonality = lambda df: df['linear_monthly']-df['trend']\n",
    "           )\n",
    "    ['seasonality']\n",
    "    .loc[:'2014']\n",
    "    .plot(c='m')\n",
    ")\n",
    "ax.axhline(0, color='r', linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we still have the seasonal waves but without the downwards trend.\n",
    "\n",
    "This simple model with dummy features appears to reasonably capture the observed seasonality, even if has limitations. For one, the model assumes fixed monthly jumps, while the actual seasonality is likely to more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gsf'></a>\n",
    "## 4. Gradual Seasonal Filtering\n",
    "\n",
    "Often seasonal effects do not come just as fixed spikes or drops. They may have gradually increasing and decreasing effects as their peak approaches and moves away in time. In such cases we may want to use a neater alternative to seasonal dummies: <font color='red'>gradual seasonal filters</font>.\n",
    "\n",
    "There are a variety of such features that we could create. \n",
    "\n",
    "### Linear\n",
    "\n",
    "A simple yet effective example are linear monthly spikes. They can be computed like so, \n",
    "\n",
    "$$ \\phi(x_i) = \\max( 1 - \\frac{| x - x^*|}{n} , 0)$$\n",
    "\n",
    "where $x$ is a given data point, $x^*$ is the peak of the current filter, and $n$ is the interval of growth/decline of the spike around the peak (e.g. 30 days). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsf_feature_maker(center_day, days=np.arange(365), year_days=365, n=30):\n",
    "    return np.maximum.reduce([\n",
    "        np.fmax(-np.abs(days - center_day)/n + 1, 0),\n",
    "        np.fmax(-np.abs(days + year_days - center_day)/n + 1, 0),  #ensures continuity across December-January\n",
    "        np.fmax(-np.abs(days - year_days - center_day)/n + 1, 0) #ensures continuity across December-January\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a filter like this translates into a variable with values the closer to 1 (or to 0), the closer (or the further) a particular date is from the given peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = gsf_feature_maker(15)\n",
    "plt.plot(base);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such gradual filters could be especially handy when dealing with daily data that typically exhibits more gradual seasonal effects. \n",
    "\n",
    "A separate filter can be assigned to each potential seasonal peak — each month in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_2008 = pd.date_range('2008-01-01', periods = 12, freq='MS')\n",
    "\n",
    "month_peaks = pd.Series(\n",
    "    index = months_2008.map(lambda data: data.month_name()),\n",
    "    data = months_2008.map(lambda data: data.replace(day=15).dayofyear)\n",
    ")\n",
    "\n",
    "month_peaks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for month in month_peaks.index:\n",
    "    peak = month_peaks.at[month]\n",
    "    ax.plot(gsf_feature_maker(peak), label=month)\n",
    "ax.legend(loc='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add this information as features and use it to model the Time Series data. \n",
    "\n",
    "It means that the effect is not felt uniformly across a month, but increases/decreases depending on the distance from the desginated centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gsf = air_df[['time_point']].copy()\n",
    "\n",
    "days = X_gsf.index.dayofyear\n",
    "\n",
    "for month in month_peaks.index:\n",
    "    peak = month_peaks.at[month]\n",
    "    X_gsf[month] = gsf_feature_maker(peak, days)\n",
    "    \n",
    "X_gsf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian\n",
    "\n",
    "Another commonly used option are filters based on a *Gaussian distribution*:\n",
    "\n",
    "$$ \\phi(x_i) = \\exp [ - \\frac{1}{2\\alpha} (x-m_i)^2]$$\n",
    "\n",
    "where $x$ is a given data point, $m_i$ is the peak of the current filter, and $\\alpha$ is a parameter responsible for the spread of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_feature_maker(center_day, days=np.arange(365), year_days = 365, alpha = 0.005):\n",
    "    return np.maximum.reduce([\n",
    "        np.fmax(np.exp(-((days - center_day)**2) / 2*alpha), 0),\n",
    "        np.fmax(np.exp(-((days + year_days - center_day)**2) / 2*alpha), 0), #ensures continuity for December-January\n",
    "        np.fmax(np.exp(-((days - year_days - center_day)**2) / 2*alpha), 0)  #ensures continuity for December-January\n",
    "        ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a filter like this translates into a variable with values the closer to 1 (or to 0), the closer (or the further) a particular date is from the given peak. However, the effect is much smoother.\n",
    "\n",
    "Depending on $\\alpha$, very steep or very gradual filters can be created.  Such gradual filters could be especially handy when dealing with daily (or even hourly) data that typically exhibits more gradual seasonal effects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = rbf_feature_maker(10, alpha = 0.005)\n",
    "plt.plot(base);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exercise: Modelling with Gradual Seasonal Filters</mark>\n",
    "\n",
    "Add Gaussian features that correspond to the different months and use them to help model the data. \n",
    "- How do the preditions compare visually to the model with seasonal dummy features?\n",
    "- How is the performance affected by the $\\alpha$ value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/rbf.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradual Seasonal Filtering summary\n",
    "\n",
    "**Pros** \n",
    "\n",
    "- simple feature engineering trick\n",
    "- all variables are interpretable\n",
    "- seasonal effects can be quantified\n",
    "- focus on filtering out the long term season, other fluctuations can be modeled separately\n",
    "\n",
    "**Cons** \n",
    "\n",
    "- the model can get a bit biased \n",
    "- the model may have issues if the seasonality changes over time (fixable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sum'></a>\n",
    "## 5. Summary\n",
    "\n",
    "Whilst modelling and forecasting time series data we have come across a few key points:\n",
    "- Seasonality matters!\n",
    "- Feature engineering is a way to turn your creativity into better models;\n",
    "- There are many ways to identify it;\n",
    "- Linear models are more powerful than people think.\n",
    "\n",
    "Next steps:\n",
    "- Alternative approaches to encoding seasonality, e.g fitting a sinusoidal curve.\n",
    "- Alternative applications, e.g. outlier detection.\n",
    "- Model evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
